{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "SkTRgmeLVqq6",
        "NEekk-YQV71Y",
        "5jlTZQuG6b7m",
        "veFwhWVeXKaZ",
        "O7OfOHT9Yrnm",
        "HDm-eOzsZDrx",
        "aGe_6To_Zi4I",
        "0AgFg2trnVwN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Generation of Data sets for Training \n",
        "\n",
        "## Generating the Proxy and Full tasks\n",
        "\n",
        "This notebook serves the purpose of extracting and downsampling patches from the HR versions of the DIV2K image data sets comprising 800 images for training and 100 for validation. Applying data augmentation to this data set, as many patches as possible with 64 x 64 pixels are extracted from the 900 images. \n",
        "\n",
        "By extracting the aforementioned patches it is possible to define two tasks with a validation and a training set each:\n",
        "\n",
        "* A proxy task, best suited for architecture search, that presents a x2 task with a reduced number of instances/patches.\n",
        "\n",
        "* A full training task, that presents 3 data pairs with different resolutions. \n",
        "\n",
        "\n",
        "It is worth noting that the total number of instances for each data task is different.\n",
        "\n",
        "### Proxy task\n",
        "\n",
        "This task is composed of two datasets. The first set has 522,939 pairs of low- and high-resolution image patches for model training. The second set contains 66650 pairs of images used for validation. These sets only consider a x2 resolution problem and no additional data augmentation is applied over the patches.\n",
        "\n",
        "The objective of this proxy is to allow a reduced computational overhead while searching for architectures, in contrast to training over larger data sets and more tasks.\n",
        "\n",
        "### Full task\n",
        "\n",
        "This task extends on the previous one by adding two more resolution differences (x3 and x4) and a data augmentation procedure. For each set of instances patches are subjected to horizontal flip, vertical flip and 90º rotation. This provides additional patter information to models, such that richer features can be extracted during training.\n",
        "\n",
        "Following these procedure results in 2M image pairs for training and 266,600 pairs for validation, with architectures requiring to reconstruct three super resolution problems: x2, x3 and x4.\n",
        "\n",
        "This task is directed at evaluating machine-crafted models exhaustively, allowing a proper contrast among different approaches. \n",
        "\n",
        "\n",
        "## Structure of this notebook.\n",
        "\n",
        "To help the reader identify different elements and their application throughout this notebook, we structured it as follows:\n",
        "\n",
        "* Library import. This section incorporates the instalation and inclusion of the necesary libraries for executing this notebook successfully.\n",
        "*   Data Loading and pre-processing. Here image paths are transformed into image arrays to be used in later sections.\n",
        "*   Creation of directories. Here the structure for image allocation in directories is defined. A compartmentalization of images in subsets is needed as the sheer quantity of files can cause troubles while importing and exporting the patches.\n",
        "* Extraction of Patches and downsampling. Here each image is subjected to the same operators that sample multiple pieces of each image, apply the data augmentation described in the full task and down sample accordingly to each resolution.\n",
        "*Data Download. While the datasets will be made public, we include a routine for downloading the data sets generated. \n"
      ],
      "metadata": {
        "id": "-3o-dhyjSlw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library import and install"
      ],
      "metadata": {
        "id": "iLspgmftVXOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection with gdrive to access local DIV2k data sets copies."
      ],
      "metadata": {
        "id": "3_Jd_1CZVcLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd \"/gdrive/MyDrive/Data_for_training/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY2loF3b6W43",
        "outputId": "91dcaefc-5e3a-4104-8aa6-1471e561af96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/Doctorate_Thesis_Coding/COMPSR-NET/Data_for_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the Patchify and Pillow libraries to handle image processing.\n",
        "\n",
        "For this we simply install the necessary libraries with !pip install."
      ],
      "metadata": {
        "id": "SkTRgmeLVqq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install patchify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv_ze3qH6eWv",
        "outputId": "9cc3d792-562b-4dc0-aa80-0e8ec0f1c685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patchify\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from patchify) (1.22.4)\n",
            "Installing collected packages: patchify\n",
            "Successfully installed patchify-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNFeQFLD6lrX",
        "outputId": "ceee6b63-7e50-494c-c514-a257a957a5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n",
        "\n",
        "We then import the necesary libraries used throughout the notebook."
      ],
      "metadata": {
        "id": "NEekk-YQV71Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import copy\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from math import floor\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "from patchify import patchify, unpatchify\n",
        "from PIL import Image\n",
        "\n",
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "7Yny4fyJ61DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "\n",
        "Here we allocate the images from the DIV2K data set as image paths.  \n",
        "\n",
        "We separate these paths into into training (800) and validation (100) sets before processing."
      ],
      "metadata": {
        "id": "5jlTZQuG6b7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_split_perc = 0.888889\n",
        "val_test_split_perc = 0.5"
      ],
      "metadata": {
        "id": "abJ-_4hwqbPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_paths = []\n",
        "val_img_paths = []\n",
        "for dirname, _, filenames in os.walk('../DIV2K_train_HR'):\n",
        "    for filename in filenames:\n",
        "        img_paths.append(os.path.join(dirname, filename))\n",
        "        \n",
        "for dirname, _, filenames in os.walk('../DIV2K_valid_HR'):\n",
        "    for filename in filenames:\n",
        "        img_paths.append(os.path.join(dirname, filename))\n",
        "        \n",
        "print('Dataset dimension: ', len(img_paths))\n",
        "\n",
        "val_img_paths = img_paths[floor(len(img_paths) * train_val_split_perc):]\n",
        "img_paths = img_paths[:floor(len(img_paths) * train_val_split_perc)]\n",
        "print('Training: ', len(img_paths))\n",
        "print('Validation: ', len(val_img_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YIyu6yq6vwT",
        "outputId": "fd6bf031-128b-4ed5-ea93-8d43c9431cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset dimension:  900\n",
            "Training:  800\n",
            "Validation:  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data pre-processing\n",
        "\n",
        "This section transforms the paths extracted from the step before into full image arrays. This way simplifying the process of working with the data."
      ],
      "metadata": {
        "id": "RyE7Z7_6ZqjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Going from local paths into full images.\n",
        "\n",
        "Here we make images more easily accesible by transforming them directly into image arrays."
      ],
      "metadata": {
        "id": "veFwhWVeXKaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrays that cover the training set. Expect a list with 800 image arrays.\n",
        "img_arrays = []\n",
        "for path in img_paths:\n",
        "  im = Image.open(path)\n",
        "  img_arrays.append(np.array(im))"
      ],
      "metadata": {
        "id": "M2boeS4CUIbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(img_arrays))"
      ],
      "metadata": {
        "id": "5Q7ms6d3UB_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4728f991-7d1b-4575-b0df-c9d2c5c3085a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrays that cover the validation set. Expect a list with 100 image arrays.\n",
        "val_img_arrays = []\n",
        "for path in val_img_paths:\n",
        "  im = Image.open(path)\n",
        "  val_img_arrays.append(np.array(im))"
      ],
      "metadata": {
        "id": "FIj7_xN_Ty7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_img_arrays))"
      ],
      "metadata": {
        "id": "jnbCuFvJT_lH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2da1bda-7c6e-4491-866e-d912908d4921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting patches from images and performing downsampling"
      ],
      "metadata": {
        "id": "ydbalHO1Mdkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we determine the size of the patches, by default our protocol uses 64 by 64 images, but other sizes could be use by a practitioner if needed."
      ],
      "metadata": {
        "id": "xml0WKw4apoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Patch size\n",
        "size = 64 #@param {type:\"number\"}"
      ],
      "metadata": {
        "id": "DiM7_IXSMdkw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we determine the resolution task we wish to generate, this can be x2, x3 or x4 as our protocol suggest. Nonetheless, larger and smaller resolutions can be generated.\n",
        "\n",
        "Note: selecting a value of 1 results in the HR patches being generated. "
      ],
      "metadata": {
        "id": "nIZJjVh0aEK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Selection of resolution.\n",
        "Task = 1 #@param {type:\"slider\", min:1, max:8, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TsiucldpZ1N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove processed dataset directories (Only use if removing the directories with data is required)\n",
        "\n",
        "The following instructions allow the quick remotion of direcories in case of encountering an issue that requires them to be generated again. "
      ],
      "metadata": {
        "id": "O7OfOHT9Yrnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/Data_set_64/"
      ],
      "metadata": {
        "id": "MBivRZ7DNzKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/Data_set_val_64/"
      ],
      "metadata": {
        "id": "PVCQsuCQohyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the required directories\n",
        "\n",
        "Here we create the directories needed to allocate the data accordingly to resolution and augmentation technique employed.\n",
        "\n",
        "Both training and validation sets follows the same general structure:\n",
        "\n",
        "```\n",
        "Data_Set/\n",
        "├─ Resolution/\n",
        "│  ├─ First subset of images/\n",
        "│  │  ├─ Original Patches/\n",
        "│  │  │  ├─ patch_0_0_0.jpg\n",
        "│  │  │  ├─ patch_0_0_1.jpg\n",
        "│  │  │  ├─ ...\n",
        "│  │  │  ├─ patch_#image_#row_#column.jpg\n",
        "│  │  ├─ Horizontal Flip Patches/\n",
        "│  │  ├─ Vertical Flip Patches/\n",
        "│  │  ├─ Rotated 90º Patches/\n",
        "│  ├─ Second subset of images/\n",
        "│  │  ├─ Original patches/\n",
        "│  │  ├─ .../\n",
        "│  ├─ .../\n",
        "│  ├─ N-th subset of images/\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDm-eOzsZDrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the directory for Training set.\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "if path.exists(f'/content/Data_set_{size}') == False:\n",
        "\n",
        "    os.mkdir(f'/content/Data_set_{size}')\n",
        "\n",
        "    if Task > 1:\n",
        "      print(Task)\n",
        "      print(f'/content/Data_set_{size}/x{Task}')\n",
        "      if path.exists(f'/content/Data_set_{size}/x{Task}') == False:\n",
        "        os.mkdir(f'/content/Data_set_{size}/x{Task}')\n",
        "        for i in range(10, 801, 10):\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Rotated_90')\n",
        "\n",
        "    else:\n",
        "      if path.exists(f'/content/Data_set_{size}/HR') == False:\n",
        "        os.mkdir(f'/content/Data_set_{size}/HR')\n",
        "        for i in range(10, 801, 10):\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Rotated_90')\n",
        "\n",
        "else:\n",
        "\n",
        "    if Task > 1:\n",
        "      if path.exists(f'/content/Data_set_{size}/x{Task}') == False:\n",
        "        os.mkdir(f'/content/Data_set_{size}/x{Task}')\n",
        "        for i in range(10, 801, 10):\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_{size}/x{Task}/{i}/Rotated_90')\n",
        "\n",
        "    else:\n",
        "      if path.exists(f'/content/Data_set_{size}/HR') == False:\n",
        "        os.mkdir(f'/content/Data_set_{size}/HR')\n",
        "        for i in range(10, 801, 10):\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_{size}/HR/{i}/Rotated_90')"
      ],
      "metadata": {
        "id": "lWDdcYP7Mdkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the directory for Validation set.\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "if path.exists(f'/content/Data_set_val_{size}') == False:\n",
        "\n",
        "    os.mkdir(f'/content/Data_set_val_{size}')\n",
        "\n",
        "    if Task > 1:\n",
        "      print(Task)\n",
        "      print(f'/content/Data_set_val_{size}/x{Task}')\n",
        "      if path.exists(f'/Data_set_val_/Data_set_{size}/x{Task}') == False:\n",
        "        os.mkdir(f'/content/Data_set_val_{size}/x{Task}')\n",
        "        for i in range(10, 101, 10):\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Rotated_90')\n",
        "\n",
        "    else:\n",
        "      if path.exists(f'/content/Data_set_val_{size}/HR') == False:\n",
        "        os.mkdir(f'/content/Data_set_val_{size}/HR')\n",
        "        for i in range(10, 101, 10):\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Rotated_90')\n",
        "\n",
        "else:\n",
        "\n",
        "    if Task > 1:\n",
        "      if path.exists(f'/content/Data_set_val_{size}/x{Task}') == False:\n",
        "        os.mkdir(f'/content/Data_set_val_{size}/x{Task}')\n",
        "        for i in range(10, 101, 10):\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/x{Task}/{i}/Rotated_90')\n",
        "\n",
        "    else:\n",
        "      if path.exists(f'/content/Data_set_val_{size}/HR') == False:\n",
        "        os.mkdir(f'/content/Data_set_val_{size}/HR')\n",
        "        for i in range(10, 101, 10):\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Original')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Flipped_Horizontally')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Flipped_Vertically')\n",
        "          os.mkdir(f'/content/Data_set_val_{size}/HR/{i}/Rotated_90')"
      ],
      "metadata": {
        "id": "zAXYEVljWmsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders = []\n",
        "for f in range(10, 801, 10):\n",
        "  folders.append(f)"
      ],
      "metadata": {
        "id": "4HBX2LFaNJkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders_val = []\n",
        "for f in range(10,101,10):\n",
        "  folders_val.append(f)"
      ],
      "metadata": {
        "id": "V7QAoAdPODJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting patches and downsampling.\n",
        "\n",
        "Here are the main blocks of code of this notebook. These focus on the extracting and downsampling of data generating in principle each of the data set tasks."
      ],
      "metadata": {
        "id": "aGe_6To_Zi4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 0\n",
        "for k in tqdm(range(len(img_arrays))):\n",
        "  one_image = img_arrays[k]\n",
        "  #Extracting patches\n",
        "  patches_img = patchify(one_image, (size,size,3), step = size)   \n",
        "  for i in range(patches_img.shape[0]):\n",
        "    for j in range(patches_img.shape[1]):\n",
        "      single_patch_img = patches_img[i, j, 0, :, :, :]\n",
        "      \n",
        "      #From patch array to image\n",
        "      patch = Image.fromarray(single_patch_img, 'RGB')\n",
        "      ##From patch array to image flipped horizontally\n",
        "      patch_FH = patch.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "      #From patch array to image flipped vertically\n",
        "      patch_FV = patch.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "      #From patch array to image rotated 90º\n",
        "      patch_R = patch.transpose(Image.ROTATE_90)\n",
        "\n",
        "\n",
        "      #Downsampling original patch by 1/2, 1/3 and 1/4 the size. \n",
        "      p_ds = patch.resize((patch.size[0]//Task, patch.size[1]//Task), Image.BICUBIC)\n",
        "      #Downsampling horizontally flipped patch by 1/2, 1/3 and 1/4 the size.\n",
        "      p_ds_fh = p_ds.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "      #Downsampling vertical flipped patch by 1/2, 1/3 and 1/4 the size.\n",
        "      p_ds_fv = p_ds.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "      #Downsampling rotated patch by 1/2, 1/3 and 1/4 the size.\n",
        "      p_ds_r = p_ds.transpose(Image.ROTATE_90)\n",
        "\n",
        "      # Saving the generated images at different directories for each corresponding image kind.\n",
        "\n",
        "      if Task == 1:\n",
        "        if k<folders[a]: \n",
        "          patch.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FH.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FV.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_R.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "        \n",
        "        else:\n",
        "          #Changing folders.\n",
        "          a+=1      \n",
        "          patch.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FH.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FV.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_R.save(f\"/content/Data_set_{size}/HR/{folders[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "      \n",
        "      else:\n",
        "        if k<folders[a]: \n",
        "          \n",
        "          p_ds.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fh.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fv.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_r.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "\n",
        "        else:\n",
        "\n",
        "          #Changing folders.\n",
        "          a+=1       \n",
        "          \n",
        "          p_ds.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fh.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fv.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_r.save(f\"/content/Data_set_{size}/x{Task}/{folders[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "\n",
        "# Uncomment to see some samples of the patches forming the image.\n",
        "#  rows = patches_img.shape[0]\n",
        "#  columns =patches_img.shape[1]\n",
        "#  fig = plt.figure()\n",
        "#  counter = 1\n",
        "#\n",
        "#  if (k+1)%1 == 0:\n",
        "#    for m in range(patches_img.shape[0]):\n",
        "#      for n in range(patches_img.shape[1]):\n",
        "#        fig.add_subplot(rows, columns, counter)\n",
        "#        counter +=1\n",
        "#        plt.imshow(patches_img[m, n, 0, :, :, :])\n",
        "#        plt.axis('off')\n",
        "#        \n",
        "#    plt.show()\n",
        "#    plt.close()"
      ],
      "metadata": {
        "id": "G9aKPW7PMdkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048ad179-ea2f-4521-9049-529a0f36e805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [12:19<00:00,  1.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 0\n",
        "for k in tqdm(range(len(val_img_arrays))):\n",
        "  one_image = val_img_arrays[k]\n",
        "  patches_img = patchify(one_image, (size,size,3), step = size)\n",
        "  for i in range(patches_img.shape[0]):\n",
        "    for j in range(patches_img.shape[1]):\n",
        "      single_patch_img = patches_img[i, j, 0, :, :, :]\n",
        "\n",
        "      patch = Image.fromarray(single_patch_img, 'RGB')\n",
        "      patch_FH = patch.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "      patch_FV = patch.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "      patch_R = patch.transpose(Image.ROTATE_90)\n",
        "      \n",
        "      p_ds = patch.resize((patch.size[0]//2, patch.size[1]//2), Image.BICUBIC)\n",
        "      p_ds_fh = p_ds.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "      p_ds_fv = p_ds.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "      p_ds_r = p_ds.transpose(Image.ROTATE_90)\n",
        "      \n",
        "      if Task == 1:\n",
        "\n",
        "        if k<folders_val[a]:\n",
        "          patch.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FH.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FV.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_R.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "\n",
        "        else:\n",
        "\n",
        "          a+=1\n",
        "          \n",
        "          patch.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FH.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_FV.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          patch_R.save(f\"/content/Data_set_val_{size}/HR/{folders_val[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "\n",
        "      else:\n",
        "        if k<folders_val[a]:\n",
        "          p_ds.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fh.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fv.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_r.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "\n",
        "        else:\n",
        "\n",
        "          a+=1\n",
        "          \n",
        "          p_ds.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Original/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fh.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Flipped_Horizontally/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_fv.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Flipped_Vertically/patch_{k}_{i}_{j}.jpg\")\n",
        "          p_ds_r.save(f\"/content/Data_set_val_{size}/x{Task}/{folders_val[a]}/Rotated_90/patch_{k}_{i}_{j}.jpg\")\n",
        "\n",
        "# Uncomment to see some samples of the patches forming the image.\n",
        "#  rows = patches_img.shape[0]\n",
        "#  columns =patches_img.shape[1]\n",
        "#  fig = plt.figure()\n",
        "#  counter = 1\n",
        "#\n",
        "#  if (k+1)%1 == 0:\n",
        "#    for m in range(patches_img.shape[0]):\n",
        "#      for n in range(patches_img.shape[1]):\n",
        "#        fig.add_subplot(rows, columns, counter)\n",
        "#        counter +=1\n",
        "#        plt.imshow(patches_img[m, n, 0, :, :, :])\n",
        "#        plt.axis('off')\n",
        "#    \n",
        "#    plt.show()\n",
        "#    plt.close()"
      ],
      "metadata": {
        "id": "Ff_LT1wKXJbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995ceecb-d1ab-47bc-d15d-12094ad9bce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing for the correct number of instances at each dataset.\n",
        "\n",
        "Here we test that the datasets are complete."
      ],
      "metadata": {
        "id": "0AgFg2trnVwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "total = 0\n",
        "for f in range(10, 801, 10):\n",
        "  FOLDER_PATH1 = f'Data_set_{size}/HR/{f}/Original'\n",
        "  FOLDER_PATH2 = f'Data_set_{size}/HR/{f}/Flipped_Horizontally'\n",
        "  FOLDER_PATH3 = f'Data_set_{size}/HR/{f}/Flipped_Vertically'\n",
        "  FOLDER_PATH4 = f'Data_set_{size}/HR/{f}/Rotated_90'\n",
        "  ROOT_PATH = '/content'\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH1)))\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH2)))\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH3)))\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH4)))\n",
        "print(total)"
      ],
      "metadata": {
        "id": "FKGrHTvuMdkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca64b912-eaa3-42ca-cca6-6eb7039d42f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2091756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "total = 0\n",
        "for f in range(10, 101, 10):\n",
        "  FOLDER_PATH1 = f'Data_set_val_{size}/HR/{f}/Original'\n",
        "  FOLDER_PATH2 = f'Data_set_val_{size}/HR/{f}/Flipped_Horizontally'\n",
        "  FOLDER_PATH3 = f'Data_set_val_{size}/HR/{f}/Flipped_Vertically'\n",
        "  FOLDER_PATH4 = f'Data_set_val_{size}/HR/{f}/Rotated_90'\n",
        "  ROOT_PATH = '/content'\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH1)))\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH2)))\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH3)))\n",
        "  total += len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH4)))\n",
        "print(total)"
      ],
      "metadata": {
        "id": "PUcH1ELNazH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c0347b-3801-4dd7-c5df-6a42b4264087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "266600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compressing data sets into ZIP files\n",
        "\n",
        "This is done so it is easier for users to download the resulting files."
      ],
      "metadata": {
        "id": "P2dPCmzvd78R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if path.exists(f'/content/Data_set_{size}_HR_train') == False:\n",
        "  os.mkdir(f'/content/Data_set_{size}_HR_train')"
      ],
      "metadata": {
        "id": "dr2w47TyFKSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,801,10):\n",
        "  directory = pathlib.Path(f\"/content/Data_set_64/HR/{i}\")\n",
        "  \n",
        "  with ZipFile(f\"/content/HR_train_{i}.zip\", \"w\") as archive:\n",
        "    for file_path in directory.rglob(\"*\"):\n",
        "      archive.write(file_path, arcname=file_path.relative_to(directory))\n"
      ],
      "metadata": {
        "id": "Tiphn89oCcWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/Data_set_64/HR_val /content/Data_set_val_64/HR"
      ],
      "metadata": {
        "id": "Q35VP0sYAyqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}